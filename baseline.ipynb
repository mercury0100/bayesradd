{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a72dd5-7a27-4f08-8caa-c3e9a97c8aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (297300 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f5e64a92ec4c92852a55b600837a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/home/jupyter/RADD/model/transformer.py:155: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=self.dtype):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/home/jupyter/RADD/model/transformer.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/home/jupyter/RADD/model/transformer.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 PPL  = 20.33\n",
      "RADD (K=8) PPL = 8.83\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "from datasets     import load_dataset\n",
    "\n",
    "# ðŸ‘‡ your RADD imports\n",
    "from load_model       import load_model\n",
    "from bayes_radd        import mc_marginal_tokenwise\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# â€”â€”â€” GPT-2 baseline (just for reference) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "tok2   = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "gpt2   = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device).eval()\n",
    "\n",
    "# â€”â€”â€” Your RADD model â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# assume load_model returns (model, noise) and youâ€™ve registered bayesRADD.mc_marginal_tokenwise\n",
    "radd, noise = load_model(\"JingyangOu/radd-t-dce\", device)\n",
    "radd.eval()\n",
    "\n",
    "# â€”â€”â€” helper to compute RADD MCâ€marginal PPL on a batch â€”â€”â€”â€”â€”â€”\n",
    "@torch.no_grad()\n",
    "def radd_ppl_on_block(x, K):\n",
    "    # x: [1, L] contiguous chunk of tokenâ€ids\n",
    "    # we will only score the last trg_len tokens\n",
    "    mc_p_true, _ = mc_marginal_tokenwise(radd, noise, x, K)  # â†’ [1, L]\n",
    "    # we only care about the tail positions (where we masked prefix)\n",
    "    # so return the sum of negative logâ€prob of those tokens\n",
    "    return -(mc_p_true.clamp(min=1e-12).log().sum().item())\n",
    "\n",
    "# â€”â€”â€” load WikiText-2 test as one long sequence â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"test\")\n",
    "raw  = \"\\n\\n\".join(test[\"text\"])\n",
    "enc2 = tok2(raw, return_tensors=\"pt\")\n",
    "ids = enc2.input_ids.to(device)  # [1, N]\n",
    "N   = ids.size(1)\n",
    "\n",
    "# â€”â€”â€” slidingâ€window settings â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "max_len, stride = 1024, 512\n",
    "nll_sum_radd, n_tok_radd = 0.0, 0\n",
    "nll_sum_gpt2, n_tok_gpt2 = 0.0, 0\n",
    "\n",
    "K = 8  # number of MC samples for RADD\n",
    "\n",
    "for i in tqdm(range(0, N, stride)):\n",
    "    begin = max(i + stride - max_len, 0)\n",
    "    end   = min(i + stride, N)\n",
    "    trg_len = end - i\n",
    "\n",
    "    block = ids[:, begin:end]            # [1, L]\n",
    "    # â€” GPT2\n",
    "    with torch.no_grad():\n",
    "        # mask out the prefix in the labels\n",
    "        labels = block.clone()\n",
    "        labels[:, :-trg_len] = -100\n",
    "        loss2 = gpt2(block, labels=labels).loss\n",
    "    nll_sum_gpt2 += loss2.item() * trg_len\n",
    "    n_tok_gpt2  += trg_len\n",
    "\n",
    "    # â€” RADD MC marginal\n",
    "    # we score that same block by calling mc_marginal_tokenwise,\n",
    "    # but only summing over the last trg_len positions\n",
    "    radd_nll = radd_ppl_on_block(block, K)\n",
    "    nll_sum_radd += radd_nll\n",
    "    n_tok_radd  += trg_len\n",
    "\n",
    "    if end == N:\n",
    "        break\n",
    "\n",
    "ppl_gpt2 = math.exp(nll_sum_gpt2 / n_tok_gpt2)\n",
    "ppl_radd = math.exp(nll_sum_radd  / n_tok_radd)\n",
    "\n",
    "print(f\"GPT-2 PPL  = {ppl_gpt2:.2f}\")\n",
    "print(f\"RADD (K={K}) PPL = {ppl_radd:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a204ed9-e1bf-43d8-b1d1-af46916a1413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (297300 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32675415cf6b4b6eb8e2677e9349a375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/581 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RADD PPL = 41.10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "from load_model import load_model\n",
    "from losses import get_loss_fn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load tokenizer & data\n",
    "tok = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"test\")\n",
    "raw  = \"\\n\\n\".join(test[\"text\"])\n",
    "ids  = tok(raw, return_tensors=\"pt\").input_ids.to(device)\n",
    "N    = ids.size(1)\n",
    "\n",
    "# Load RADD and analytic DCE loss\n",
    "model, noise = load_model(\"JingyangOu/radd-t-dce\", device)\n",
    "model.eval()\n",
    "dce_loss_fn = get_loss_fn(\n",
    "    noise,\n",
    "    model.config.tokens + 1,  # include [MASK]\n",
    "    train=False,\n",
    "    loss_type=\"lambda_DCE\"\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def vanilla_radd_ppl(seq_len=1024, stride=512, max_batches=None):\n",
    "    total_nll, total_toks = 0.0, 0\n",
    "    batches = 0\n",
    "\n",
    "    for i in tqdm(range(0, N, stride)):\n",
    "        if max_batches and batches >= max_batches:\n",
    "            break\n",
    "\n",
    "        begin = max(i + stride - seq_len, 0)\n",
    "        end   = min(i + stride, N)\n",
    "        trg_len = end - i\n",
    "\n",
    "        block = ids[:, begin:end]     # [1, L]\n",
    "        L = block.size(1)\n",
    "\n",
    "        # --- get *sum* NLL over the block (dce_loss_fn returns sum NLL) ---\n",
    "        block_nll = dce_loss_fn(model, block).item()\n",
    "\n",
    "        # --- isolate the tail (last trg_len tokens) ---\n",
    "        tail_nll = block_nll * (trg_len / L)\n",
    "\n",
    "        total_nll  += tail_nll\n",
    "        total_toks += trg_len\n",
    "        batches    += 1\n",
    "\n",
    "        if end == N:\n",
    "            break\n",
    "\n",
    "    # average NLL per token, then exponentiate\n",
    "    avg_nll = total_nll / total_toks\n",
    "    return math.exp(avg_nll)\n",
    "\n",
    "# Run it!\n",
    "ppl_vanilla = vanilla_radd_ppl(seq_len=1024, stride=512, max_batches=100)\n",
    "print(f\"Vanilla RADD PPL = {ppl_vanilla:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
